import pandas as pd

metadata_file = config["input_meta"]
metadata = pd.read_csv(metadata_file, sep="\t").set_index(["batch","sample"])
nparallel = config['nparallel'] if 'nparallel' in config else 40
tmpdir=config['tmpdir']


batch_ids, sample_names = list(zip(*metadata.index))
batch_ids = set(batch_ids)
sample_names = set(sample_names)
print("batch_ids", batch_ids)
print("sample_names", sample_names)

SPLIT_OUT="split-out"
SPLITS = [str(x+1) for x in range(nparallel)]

REF= config['ref']
CODEC_root = config['codec_root']

DEMUX=f"{CODEC_root}/build/codec demux"
TRIM=f"{CODEC_root}/build/codec trim"
CALL_BIN=f"{CODEC_root}/build/codec call"
AGG_METRICS_SCRIPT=f"{CODEC_root}/snakemake/script/mscodec_get_byproduct_metrics.py"
AGG_LOG_SCRIPT=f"{CODEC_root}/snakemake/script/agg_log.py"
MSCODEC_FIX_PROPER_PAIR_FLAG=f"{CODEC_root}/snakemake/script/mscodec_fixproperpair_and_cal_byprod.py"
MSCODEC_INTEGRATE_PAIR=f"{CODEC_root}/snakemake/script/mscodec_integrate_pairs.py"
MSCODEC_GET_METHYL_READ=f"{CODEC_root}/snakemake/script/mscodec_separate_methreads.py"
SPLIT_SCRIPT=f"{CODEC_root}/snakemake/script/fastqsplit.pl"

JAVA_PARAMS = "-Xmx30g -Djava.io.tmpdir=/tmp"
BWA = config['bwa']
PICARD = f"java {JAVA_PARAMS} -jar $PICARD"
GATK = f"{config['gatk4']} --java-options \"{JAVA_PARAMS}\""
GATK3 = f"{config['gatk3']} --java-options \"{JAVA_PARAMS}\""
FGBIO = f"java {JAVA_PARAMS} -jar {config['fgbio']}"

PROJECT= config["project"]
DETECT_DIR="detect"
ACCU_OUT="sfc"
ADAPTER_TRIM_OUT="adap_trimmed"
TRIM_MAPPED="mapped"
Metrics_OUT="metrics"
workdir: config["cwd"]

batch_to_fastq = metadata.reset_index().groupby('batch').agg({'fastq1' : set, 'fastq2': set})
batch_to_samplesheet = metadata.reset_index().groupby('batch').agg({'sample_sheet' : set})
batch_to_samples = metadata.reset_index().groupby('batch').agg({'sample' : set})
sample_to_batch = metadata.reset_index().groupby('sample').agg({'batch' : set})
#sample_to_germvcf = metadata.reset_index().groupby('sample').agg({'germline_vcf' : set})
sample_to_germbam = metadata.reset_index().groupby('sample').agg({'germline_bam' : set})
rule SplitFastq1:
    input:
         lambda wildcards: batch_to_fastq.loc[wildcards.id]['fastq1']
    params:
          nsplit = nparallel,
          prefix = expand("/broad/hptmp/ruolin/{prjdir}/{{id}}_split_r1", prjdir=PROJECT)
    output:
          split = temp(expand("/broad/hptmp/ruolin/{prjdir}/{{id}}_split_r1.{ss}.fastq", ss = SPLITS, prjdir=PROJECT))
    resources:
        mem = 8,
        runtime = 24
    shell:
        """
        zcat {input} | {SPLIT_SCRIPT} {params.prefix} {params.nsplit}
        """

rule SplitFastq2:
    input:
         lambda wildcards: batch_to_fastq.loc[wildcards.id]['fastq2']
    params:
          nsplit = nparallel,
          prefix = expand("/broad/hptmp/ruolin/{prjdir}/{{id}}_split_r2", prjdir=PROJECT)
    output:
          split = temp(expand("/broad/hptmp/ruolin/{prjdir}/{{id}}_split_r2.{ss}.fastq", ss = SPLITS, prjdir=PROJECT))
    resources:
        mem = 8,
        runtime = 24
    shell:
         """
         zcat {input} | {SPLIT_SCRIPT} {params.prefix} {params.nsplit}
         """

for batch in batch_ids:
    rule:
        name: f"Demux_{batch}"
        input:
          read1 = f"/broad/hptmp/ruolin/{PROJECT}/{batch}_split_r1.{{ss}}.fastq",
          read2 = f"/broad/hptmp/ruolin/{PROJECT}/{batch}_split_r2.{{ss}}.fastq",
          sample_sheet = batch_to_samplesheet.loc[batch]['sample_sheet']
        output:
          read1 = temp(expand("demux/%s_split.{{ss}}.{index}.1.fastq.gz" % batch, index = batch_to_samples.loc[batch]['sample'])),
          read2 = temp(expand("demux/%s_split.{{ss}}.{index}.2.fastq.gz" % batch,  index = batch_to_samples.loc[batch]['sample'])),
        resources:
         mem = 8,
         runtime = 24
        wildcard_constraints:
           ss = "[0-9]+"
        params:
         outprefix = f"demux/{batch}_split.{{ss}}",
         log = f"demux/{batch}_split.{{ss}}.log",
         ref = {REF}
        shell:
         """
             {DEMUX} -1 {input.read1} -2 {input.read2} -p {input.sample_sheet} -o {params.outprefix} > {params.log}
         """


rule Trim:
    input:
        read1 = "demux/{batch_id}_split.{ss}.{index}.1.fastq.gz",
        read2 = "demux/{batch_id}_split.{ss}.{index}.2.fastq.gz",
    output:
        trim = temp("adap_trimmed/{batch_id}_split.{ss}.{index}.trim.bam"),
        log = temp("adap_trimmed/{batch_id}_split.{ss}.{index}.trim.log"),
    params:
        adap_type = "custom_primer_v2",
        outprefix = "adap_trimmed/{batch_id}_split.{ss}.{index}",
        rgsm = "{index}"
    resources:
        mem = 8,
        runtime = 24
    wildcard_constraints:
        index = "[0-9a-zA-Z_-]+",
        ss = "[0-9]+"
    shell:
        """
            {TRIM} -1 {input.read1} -2 {input.read2} -o {params.outprefix} -u 3 -U 3 -f 2 -t 2 -s {params.rgsm}  > {output.log}
        """

rule MergeLogSplit:
    input:
        expand(ADAPTER_TRIM_OUT + "/{{batch_id}}_split.{ss}.{{index}}.trim.log", ss=SPLITS),
    output:
        ADAPTER_TRIM_OUT + "/{batch_id}.{index}.trim.log"
    wildcard_constraints:
        batch_id = "[0-9a-zA-Z_-]+",
        index = "[0-9a-zA-Z_-]+"
    resources:
        mem = 8,
        tuntime = 24,
        ncores = 1
    shell:
        """
         python {AGG_LOG_SCRIPT} {input} {output}
        """

rule MethylAlign:
    input:
        bam = "adap_trimmed/{batch_id}_split.{ss}.{index}.trim.bam"
    output:
        pairbam = temp("tmp/{batch_id}_split.{ss}.{index}.paired_reads.bam"),
        #pfastq = temp("tmp/{batch_id}_split.{ss}.{index}.protected_reads.fastq.gz"),
        #cfastq = temp("tmp/{batch_id}_split.{ss}.{index}.converted_reads.fastq.gz"),
        singlePstrand = temp("tmp/{batch_id}_split.{ss}.{index}.protected_strand.fastq.gz"),
        singleOstrand = temp("tmp/{batch_id}_split.{ss}.{index}.converted_strand.fastq.gz"),
        log = temp("tmp/{batch_id}_split.{ss}.{index}.msalign.log")
    params:
        outprefix = "tmp/{batch_id}_split.{ss}.{index}",
        rg_str = "@RG\tID:{batch_id}\tSM:bar{index}",
    resources:
        mem = 16,
        runtime = 72,
        ncores = 1
    shell:
        """
        {CODEC_root}/build/codec ms-align -b {input.bam} -o {params.outprefix} -r {REF} -q 30 -d 12 -R '{params.rg_str}' -i {params.outprefix} > {output.log}
        """

rule MergeMethylAlignLog:
    input:
        logs = expand("tmp/{{batch_id}}_split.{ss}.{{index}}.msalign.log", ss=SPLITS),
    output:
        outfile = Metrics_OUT + "/{batch_id}.{index}.msalign.log"
    wildcard_constraints:
        batch_id = "[0-9a-zA-Z_-]+",
        index = "[0-9a-zA-Z_-]+"
    resources:
        mem = 4,
        runtime = 24,
        ncores = 1
    run:
        total = pd.DataFrame()
        for f in input.logs:
            df = pd.read_csv(f, sep='\t')
            total = pd.concat([total, df])
        # do a rowSum of every column in the dataframe
        total = total.sum(axis=0)
        total.to_csv(output.outfile, sep='\t', header=False)

rule MergeBamSplit:
    input:
          expand("tmp/{{batch_id}}_split.{ss}.{{index}}.paired_reads.bam", ss=SPLITS),
    output:
          merged=temp("tmp/{batch_id}.{index}.ds_reads.merged.bam"),
          sorted=temp("tmp/{batch_id}.{index}.ds_reads.sorted.bam")
    wildcard_constraints:
        batch_id = "[0-9a-zA-Z_-]+",
        index = "[0-9a-zA-Z_-]+"
    resources:
          mem = 8,
          runtime = 72,
          ncores = config['ncores']
    shell:
         """
         samtools merge -@ {resources.ncores} {output.merged} {input} && samtools sort -@ {resources.ncores} {output.merged} -o {output.sorted}
         """

rule BWASinglePstrand:
    input:
        "tmp/{batch_id}_split.{ss}.{index}.protected_strand.fastq.gz",
    output:
        temp("tmp/{batch_id}_split.{ss}.{index}.single_protected_strand.bwa.bam")
    wildcard_constraints:
        batch_id = "[0-9a-zA-Z_-]+",
        index = "[0-9a-zA-Z_-]+"
    resources:
        mem = 8,
        runtime = 72,
    params:
        reference = REF,
    shell:
        """
        bwa mem -K 100000000 -t 1 -p -Y {params.reference} {input} | samtools view -bS - -o {output}
        """

rule MergeSinglePstrandBamSplit:
    input:
        expand("tmp/{{batch_id}}_split.{ss}.{{index}}.single_protected_strand.bwa.bam", ss=SPLITS),
    output:
        temp("tmp/{batch_id}.{index}.protected_strand_only.aligned.bam"),
    wildcard_constraints:
        batch_id = "[0-9a-zA-Z_-]+",
        index = "[0-9a-zA-Z_-]+"
    resources:
        mem = 8,
        runtime = 72,
        ncores = config['ncores']
    shell:
        """
        samtools merge -@ {resources.ncores} {output} {input} 
        """

rule BismarkSingleCStrand:
    input:
        "tmp/{batch_id}_split.{ss}.{index}.converted_strand.fastq.gz",
    output:
        temp("tmp/{batch_id}_split.{ss}.{index}.single_converted_strand.bismark.bam")
    params:
        sample = "{batch_id}_split.{ss}.{index}.single_converted_strand.bismark"
    wildcard_constraints:
        batch_id = "[0-9a-zA-Z_-]+",
        index = "[0-9a-zA-Z_-]+"
    resources:
        mem = 8,
        runtime = 72,
        ncores = config['ncores']
    shell:
        """
        bismark ~/ruolin/database/genome/human/hg38/bismark_bowtie2_ref/ {input} --basename {params.sample} -o tmp --pbat --local -p {resources.ncores}
        """

rule MergeSingleCStrandBismarkBam:
    input:
        expand("tmp/{{batch_id}}_split.{ss}.{{index}}.single_converted_strand.bismark.bam", ss=SPLITS),
    output:
        temp("tmp/{batch_id}.{index}.converted_strand_only.bismark.bam"),
    wildcard_constraints:
        batch_id = "[0-9a-zA-Z_-]+",
        index = "[0-9a-zA-Z_-]+"
    resources:
        mem = 8,
        runtime = 24,
        ncores = config['ncores']
    shell:
        """
        samtools merge {input} -o {output} -@ {resources.ncores}
        """

# rule IntergrateSingleStrand1:
#     input:
#         pstrand="tmp/{batch_id}.{index}.protected_strand_only.aligned.bam",
#         ostrand="tmp/{batch_id}.{index}.converted_strand_only.bismark.bam"
#     output:
#         temp = temp("tmp/{batch_id}.{index}.singlestrands_merged_sortbyname.bam"),
#     wildcard_constraints:
#         batch_id = "[0-9a-zA-Z_-]+",
#         index = "[0-9a-zA-Z_-]+"
#     resources:
#         mem = 8,
#         runtime = 48,
#         ncores = config['ncores']
#     shell:
#         """
#         samtools merge {input.pstrand} {input.ostrand} -o - -@ {resources.ncores} | samtools sort -n -o {output.temp} -@ {resources.ncores}
#         """

# rule IntergrateSingleStrand2:
#     input:
#         temp = "tmp/{batch_id}.{index}.singlestrands_merged_sortbyname.bam",
#     output:
#         singlestrand_out = temp("tmp/{batch_id}.{index}.singlestrands.sortbyname.bam"),
#         rescued_paired_out = temp("tmp/{batch_id}.{index}.rescued_doublestrands.bam"),
#         log = Metrics_OUT + "/{batch_id}.{index}.rescure.log"
#     wildcard_constraints:
#         batch_id = "[0-9a-zA-Z_-]+",
#         index = "[0-9a-zA-Z_-]+"
#     resources:
#         mem = 8,
#         runtime = 72,
#         ncores = 1
#     shell:
#         """
#         cat {input.temp} | python {MSCODEC_INTEGRATE_PAIR} | samtools fixmate -p - - | {MSCODEC_FIX_PROPER_PAIR_FLAG} {output.rescued_paired_out} {output.singlestrand_out} > {output.log}
#         """

# rule MergeCorrectPairs:
#     input:
#         rescued_paired_out = "tmp/{batch_id}.{index}.rescued_doublestrands.bam",
#         first_paired_out="tmp/{batch_id}.{index}.firstpass.ds_reads.sorted.bam"
#     output:
#         tmpout = temp("tmp/{batch_id}.{index}.rescuded_doublestrands.sorted.bam"),
#         merged = temp("tmp/{batch_id}.{index}.all_ds_reads.sorted.bam")
#     wildcard_constraints:
#         batch_id = "[0-9a-zA-Z_-]+",
#         index = "[0-9a-zA-Z_-]+"
#     resources:
#         mem = 8,
#         runtime = 48,
#         ncores = config['ncores']
#     shell:
#         """
#         samtools sort -@ {resources.ncores} {input.rescued_paired_out} -o {output.tmpout} && \
#         samtools merge -@ {resources.ncores} {output.merged} {input.first_paired_out} {output.tmpout}
#         """

rule SortSinglePStrands:
    input:
        pstrand="tmp/{batch_id}.{index}.protected_strand_only.aligned.bam",
    output:
        temp("tmp/{batch_id}.{index}.protected_strand_only.sorted.bam")
    wildcard_constraints:
        batch_id = "[0-9a-zA-Z_-]+",
        index = "[0-9a-zA-Z_-]+"
    resources:
        mem = 8,
        runtime = 48,
        ncores = 1
    shell:
        """
        samtools sort -@ 1 {input} -o {output}
        """

rule SortSingleCStrands:
    input:
        pstrand="tmp/{batch_id}.{index}.converted_strand_only.bismark.bam",
    output:
        temp("tmp/{batch_id}.{index}.converted_strand_only.sorted.bam")
    wildcard_constraints:
        batch_id = "[0-9a-zA-Z_-]+",
        index = "[0-9a-zA-Z_-]+"
    resources:
        mem = 8,
        runtime = 48,
        ncores = 1
    shell:
        """
        samtools sort -@ 1 {input} -o {output}
        """

rule MergeDoubleStrandReadsGroup:
    input:
         lambda wildcard: expand("tmp/{batch_id}.{{index}}.ds_reads.sorted.bam", batch_id = sample_to_batch.loc[wildcard.index]['batch'])
    output:
          bam = temp("tmp/{index}.all_ds_reads.sorted.bam")
    resources:
             mem = 8,
             ncores = config['ncores'],
             runtime = 72,
    wildcard_constraints:
        index = "[0-9a-zA-Z_-]+",
    run:
        if len(input) == 1:
            shell("cp {input} {output.bam}")
        else:
            shell("samtools merge -@ {resources.ncores} {output.bam} {input}")

rule MarkPairedDuplicates:
    input:
        bam="tmp/{index}.all_ds_reads.sorted.bam"
    resources:
        mem = 16,
        runtime = 48,
    output:
        bam = "ms-aligned-bam/{index}.ds_reads.markdup.bam",
        met = "metrics/{index}.ds_reads.marked_duplicates.txt",
    shell:
        """
        {PICARD} MarkDuplicates I={input.bam} O={output.bam} M={output.met} CREATE_INDEX=true TAG_DUPLICATE_SET_MEMBERS=true TAGGING_POLICY=All
        """

rule MergeSinglePStrandReadGroup:
    input:
        lambda wildcard: expand("tmp/{batch_id}.{{index}}.protected_strand_only.sorted.bam", batch_id = sample_to_batch.loc[wildcard.index]['batch'])
    output:
        bam = temp("tmp/{index}.protected_strand_only.sorted.bam")
    resources:
        mem = 8,
        runtime = 72,
        ncores = 1
    wildcard_constraints:
        index = "[0-9a-zA-Z_-]+",
    run:
        if len(input) == 1:
            shell("cp {input} {output.bam}")
        else:
            shell("samtools merge -@ {resources.ncores} {output.bam} {input}")

rule MergeSingleCStrandReadGroup:
    input:
        lambda wildcard: expand("tmp/{batch_id}.{{index}}.converted_strand_only.sorted.bam", batch_id = sample_to_batch.loc[wildcard.index]['batch'])
    output:
        bam = temp("tmp/{index}.converted_strand_only.sorted.bam")
    resources:
        mem = 8,
        runtime = 72,
        ncores = 1
    wildcard_constraints:
        index = "[0-9a-zA-Z_-]+",
    run:
        if len(input) == 1:
            shell("cp {input} {output.bam}")
        else:
            shell("samtools merge -@ {resources.ncores} {output.bam} {input}")

rule MarkSinglePStrandDuplicates:
    input:
        bam = "tmp/{index}.protected_strand_only.sorted.bam"
    output:
        bam = "ms-aligned-bam/{index}.protected_strand_only.markdup.bam",
        met = "metrics/{index}.protected_strand_only.marked_duplicates.txt"
    resources:
        mem = 32,
        runtime = 48,
    shell:
        """
        {PICARD} MarkDuplicates I={input.bam} O={output.bam} M={output.met} CREATE_INDEX=true TAG_DUPLICATE_SET_MEMBERS=true TAGGING_POLICY=All
        """

rule MarkSingleCStrandDuplicates:
    input:
        bam = "tmp/{index}.converted_strand_only.sorted.bam"
    output:
        bam = "ms-aligned-bam/{index}.converted_strand_only.markdup.bam",
        met = "metrics/{index}.converted_strand_only.marked_duplicates.txt"
    resources:
        mem = 32,
        runtime = 48,
    shell:
        """
        {PICARD} MarkDuplicates I={input.bam} O={output.bam} M={output.met} CREATE_INDEX=true TAG_DUPLICATE_SET_MEMBERS=true TAGGING_POLICY=All
        """


rule OutputConvertedStrand:
    input:
        bam = "ms-aligned-bam/{index}.ds_reads.markdup.bam",
    resources:
        mem = 4,
        runtime = 48,
    output:
        methylbam = "ms-aligned-bam/{index}.converted_strand.duplicate_removed.bam",
        regularbam = "ms-aligned-bam/{index}.protected_strand.duplicate_removed.bam",
    shell:
        """
        {MSCODEC_GET_METHYL_READ} {input.bam} {output.methylbam} {output.regularbam}
        """

rule CollectInsertSizeMetrics:
    input:
        bam = "ms-aligned-bam/{index}.ds_reads.markdup.bam",
    output:
          txt = Metrics_OUT + "/{index}.raw.insert_size_metrics.txt",
          hist = Metrics_OUT + "/{index}.raw.insert_size_histogram.pdf"
    shell:
         """
         {PICARD} CollectInsertSizeMetrics I={input.bam} O={output.txt} H={output.hist} M=0.5 W=600 DEVIATIONS=100
         """

rule BismarkExtractMethylation:
    input:
        "ms-aligned-bam/{index}.converted_strand.duplicate_removed.bam",
    output:
        "bismark/{index}.converted_strand.duplicate_removed.CpG_report.txt.gz",
        "bismark/{index}.converted_strand.duplicate_removed.bedGraph.gz.bismark.zero.cov"
    resources:
        mem = 8,
        runtime = 96,
        ncores = config['ncores']
    shell:
        """
        bismark_methylation_extractor -s --cytosine_report --genome_folder /xchip/bloodbiopsy/ruolin/database/genome/human/hg38/  \
            --bedGraph --gzip {input} -o bismark --zero_based --comprehensive --parallel {resources.ncores}
        """
rule RemoveDuplciateInSingleConvertedStrand:
    input:
        "ms-aligned-bam/{index}.converted_strand_only.markdup.bam",
    output:
        temp("ms-aligned-bam/{index}.converted_strand_only.duplicate_removed.bam"),
    resources:
        mem = 4,
        runtime = 96,
    shell:
        """
        samtools view -F 1024 {input} -b -o {output}
        """

rule BismarkExtractMethylationSingleStrand:
    input:
        "ms-aligned-bam/{index}.converted_strand_only.duplicate_removed.bam",
    output:
        "bismark/{index}.converted_strand_only.duplicate_removed.CpG_report.txt.gz",
        "bismark/{index}.converted_strand_only.duplicate_removed.bedGraph.gz.bismark.zero.cov"
    resources:
        mem = 8,
        runtime = 96,
        ncores = config['ncores']
    shell:
        """
        bismark_methylation_extractor -s --cytosine_report --genome_folder /xchip/bloodbiopsy/ruolin/database/genome/human/hg38/  \
            --bedGraph --gzip {input} -o bismark --zero_based --comprehensive --parallel {resources.ncores}
        """

rule BismarkMethylationBedGraphMerge:
    input:
        correct="bismark/{index}.converted_strand.duplicate_removed.bedGraph.gz.bismark.zero.cov",
        byproduct="bismark/{index}.converted_strand_only.duplicate_removed.bedGraph.gz.bismark.zero.cov"
    output:
        "bismark/{index}.converted_strand.duplicate_removed.nodecoy.strand_merge.cov"
    resources:
        mem = 4,
        runtime = 24,
    shell:
        """
        cat {input.correct} {input.byproduct} | sort -V -k1,1 -k2,2 | awk 'length($1) <= 5 && $1 != "chrM" {{print $0}}' \
        | /xchip/bloodbiopsy/ruolin/apps/bedtools2/bedtools merge -c 5,6 -o sum,sum > {output}
        """


# rule CpGIslandSummary:
#     input:
#         "bismark/{index}.converted_strand.duplicate_removed.bedGraph.gz.bismark.zero.cov"
#     output:
#         "bismark/{index}.converted_strand.duplicate_removed.bedGraph.gz.bismark.zero.nodocy.cov"


rule CollectByProductMetrics:
    input:
        trim_log = ADAPTER_TRIM_OUT + "/{batch_id}.{index}.trim.log",
        ms_log = Metrics_OUT + "/{batch_id}.{index}.msalign.log",
    output:
        met = Metrics_OUT + "/byproduct/{batch_id}.{index}.byproduct.txt"
    params:
        sid = "{index}"
    wildcard_constraints:
        index = "[0-9a-zA-Z_-]+"
    resources:
        mem = 8,
        runtime = 24,
    shell:
        """
           python {AGG_METRICS_SCRIPT} {params.sid} {input.trim_log} {input.ms_log}  > {output.met} 
        """
rule GroupReadByUMI:
    input:
        "tmp/{index}.ds_reads.sorted.bam"
    output:
        bam = temp("groupbyumi/{index}.GroupedByUmi.bam"),
        histogram = Metrics_OUT + "/{index}.umiHistogram.txt"
    resources:
        mem = 8,
        runtime = 32,
        ncores = 2
    shell:
        """
        {FGBIO} --compression 1 --async-io \
             GroupReadsByUmi \
            -i {input} \
            -o {output.bam} \
            -f {output.histogram} \
            -m 0 \
            --strategy=paired
        """

rule FgbioCollapseReadFamilies:
    input:
        "groupbyumi/{index}.GroupedByUmi.bam",
    output:
        "tmp/{index}.mol_consensus.bam"
    params:
        rg = "{index}"
    resources:
        mem = 8,
        runtime = 48,
        ncores = config['ncores']
    shell:
        """

        {FGBIO} --compression 1 CallMolecularConsensusReads \
            -i {input} \
            -o {output} \
            -p {params.rg} \
            --threads {resources.ncores} \
            --consensus-call-overlapping-bases false \
            -M 1
        """

#rule AlignMolecularConsensusReads:
#    input:
#        "tmp/{index}.mol_consensus.bam"
#    output:
#        temp("tmp/{index}.mol_consensus.aligned.bam")
#    params:
#        reference = REF,
#    resources:
#        mem = 6,
#        runtime = 96,
#        ncores = 4
#    shell:
#        """
#        samtools fastq {input} \
#        | bwa mem -K 100000000 -t {resources.ncores} -p -Y {params.reference} - | samtools view -bS - -o {output}
#        """
#
#rule MergeAndSortMoleculeConsensusReads:
#    input:
#        mapped = "tmp/{index}.mol_consensus.aligned.bam",
#        unmapped =  "tmp/{index}.mol_consensus.bam"
#    output:
#        bam = "consensus/{index}.mol_consensus.aligned.bam",
#        bai = "consensus/{index}.mol_consensus.aligned.bam.bai"
#    params:
#        reference = REF,
#    resources:
#        mem = 4,
#        runtime = 96,
#        ncores = config['ncores']
#    shell:
#        """
#        {FGBIO} --compression 0 --async-io ZipperBams \
#            -i {input.mapped} \
#            --unmapped {input.unmapped} \
#             --ref {params.reference} \
#             --tags-to-reverse Consensus \
#             --tags-to-revcomp Consensus \
#        | samtools sort - -o {output.bam} -O BAM -@ {resources.ncores} && samtools index {output.bam} -@ {resources.ncores}
#        """

